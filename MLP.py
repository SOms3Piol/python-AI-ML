# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EIfwTn5fU7EfqWjT2wZv8Vf5tLUHNtGz
"""

import numpy as np

class NeuralNetwork:

  def __init__(self ,input_n , hidden_n , output_n):
      self.w1 = np.random.rand(hidden_n, input_n)
      self.w2 = np.random.rand(hidden_n, output_n)
      self.b1 = np.random.rand(hidden_n)
      self.b2 = np.random.rand(output_n)


  def sigmoid(self, x):
      return 1 / ( 1 + np.exp(-x) )

  def sigmoid_derivative(self, x):
      return x * (1 - x)

  def forward(self , X):
      l1 = np.dot(X, self.w1.T) + self.b1
      self.ol1 = self.sigmoid(l1)
      l2 = np.dot(self.ol1 , self.w2) + self.b2
      self.ol2 = self.sigmoid(l2)
      return self.ol2

  def backward(self, X, y , output , learning_rate = 1.0):
      output_err = y - output
      output_delta = output_err * self.sigmoid_derivative(output)

      output_delta = output_delta.reshape(-1,1)

      error_h = np.dot(output_delta , self.w2.T)
      delta_h = error_h * self.sigmoid_derivative(self.ol1)

      self.w2 += np.dot(self.ol1.T, output_delta) * learning_rate
      self.b2 += np.sum(output_delta , axis=0)
      self.w1 += np.dot(delta_h.T , X) * learning_rate
      self.b1 += np.sum(delta_h , axis=0) * learning_rate


  def train(self , X , y , iteration):
      for epoch in range(iteration):
          output = self.forward(X)
          self.backward(X , y , output)
          if(epoch % 100 == 0):
              print(f" Epoch: {epoch} , Loss: {np.mean((y - output) ** 2)}")

x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])

y = np.array([[0], [1], [1], [0]])

nn = NeuralNetwork(2, 2, 1)
nn.train(x, y, 1000)

print("\n Testing: \n")
for i in range(len(x)):
  pred = nn.forward(x[i])
  print(f"Input: {x[i]} pred: {round(pred[0])} target: {y[i]}")

